{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.lax as lax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "import optax  # https://github.com/deepmind/optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(arrays, batch_size):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = np.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = np.random.permutation(indices)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end <= dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size\n",
    "\n",
    "\n",
    "def get_data(dataset_size, *, key):\n",
    "    t = jnp.linspace(0, 2 * math.pi, 16)\n",
    "    offset = jrandom.uniform(key, (dataset_size, 1), minval=0, maxval=2 * math.pi)\n",
    "    x1 = jnp.sin(t + offset) / (1 + t)\n",
    "    x2 = jnp.cos(t + offset) / (1 + t)\n",
    "    y = jnp.ones((dataset_size, 1))\n",
    "\n",
    "    half_dataset_size = dataset_size // 2\n",
    "    x1 = x1.at[:half_dataset_size].multiply(-1)\n",
    "    y = y.at[:half_dataset_size].set(0)\n",
    "    x = jnp.stack([x1, x2], axis=-1)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(eqx.Module):\n",
    "    hidden_size: int\n",
    "    cell: eqx.Module\n",
    "    linear: eqx.nn.Linear\n",
    "    bias: jax.Array\n",
    "\n",
    "    def __init__(self, in_size, out_size, hidden_size, *, key):\n",
    "        ckey, lkey = jrandom.split(key)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell = eqx.nn.GRUCell(in_size, hidden_size, key=ckey)\n",
    "        self.linear = eqx.nn.Linear(hidden_size, out_size, use_bias=False, key=lkey)\n",
    "        self.bias = jnp.zeros(out_size)\n",
    "\n",
    "    def __call__(self, input):\n",
    "        hidden = jnp.zeros((self.hidden_size,))\n",
    "\n",
    "        def f(carry, inp):\n",
    "            return self.cell(inp, carry), None\n",
    "\n",
    "        out, _ = lax.scan(f, hidden, input)\n",
    "        # sigmoid because we're performing binary classification\n",
    "        return jax.nn.sigmoid(self.linear(out) + self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0, loss=0.7216176986694336\n",
      "step=1, loss=0.6902147531509399\n",
      "step=2, loss=0.6979550123214722\n",
      "step=3, loss=0.6814358234405518\n",
      "step=4, loss=0.7044166326522827\n",
      "step=5, loss=0.6944574117660522\n",
      "step=6, loss=0.6910380125045776\n",
      "step=7, loss=0.6976555585861206\n",
      "step=8, loss=0.6890456080436707\n",
      "step=9, loss=0.6951369047164917\n",
      "step=10, loss=0.6963343620300293\n",
      "step=11, loss=0.6905266046524048\n",
      "step=12, loss=0.6929740309715271\n",
      "step=13, loss=0.6984668970108032\n",
      "step=14, loss=0.6953421235084534\n",
      "step=15, loss=0.6912515163421631\n",
      "step=16, loss=0.6906266212463379\n",
      "step=17, loss=0.6928770542144775\n",
      "step=18, loss=0.6971147060394287\n",
      "step=19, loss=0.6919533014297485\n",
      "final_accuracy=0.5\n"
     ]
    }
   ],
   "source": [
    "dataset_size=10000\n",
    "batch_size=32\n",
    "learning_rate=3e-3\n",
    "steps=20\n",
    "hidden_size=16\n",
    "depth=1\n",
    "seed=5678\n",
    "\n",
    "data_key, model_key = jrandom.split(jrandom.PRNGKey(seed), 2)\n",
    "xs, ys = get_data(dataset_size, key=data_key)\n",
    "iter_data = dataloader((xs, ys), batch_size)\n",
    "\n",
    "model = RNN(in_size=2, out_size=1, hidden_size=hidden_size, key=model_key)\n",
    "\n",
    "@eqx.filter_value_and_grad\n",
    "def compute_loss(model, x, y):\n",
    "    pred_y = jax.vmap(model)(x)\n",
    "    # Trains with respect to binary cross-entropy\n",
    "    return -jnp.mean(y * jnp.log(pred_y) + (1 - y) * jnp.log(1 - pred_y))\n",
    "\n",
    "# Important for efficiency whenever you use JAX: wrap everything into a single JIT\n",
    "# region.\n",
    "@eqx.filter_jit\n",
    "def make_step(model, x, y, opt_state):\n",
    "    loss, grads = compute_loss(model, x, y)\n",
    "    updates, opt_state = optim.update(grads, opt_state)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return loss, model, opt_state\n",
    "\n",
    "optim = optax.adam(learning_rate)\n",
    "opt_state = optim.init(model)\n",
    "for step, (x, y) in zip(range(steps), iter_data):\n",
    "    loss, model, opt_state = make_step(model, x, y, opt_state)\n",
    "    loss = loss.item()\n",
    "    print(f\"step={step}, loss={loss}\")\n",
    "\n",
    "pred_ys = jax.vmap(model)(xs)\n",
    "num_correct = jnp.sum((pred_ys > 0.5) == ys)\n",
    "final_accuracy = (num_correct / dataset_size).item()\n",
    "print(f\"final_accuracy={final_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, grads = compute_loss(model, x, y)\n",
    "updates, opt_state = optim.update(grads, opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  hidden_size=None,\n",
       "  cell=GRUCell(\n",
       "    weight_ih=f32[48,2],\n",
       "    weight_hh=f32[48,16],\n",
       "    bias=f32[48],\n",
       "    bias_n=f32[16],\n",
       "    input_size=2,\n",
       "    hidden_size=16,\n",
       "    use_bias=True\n",
       "  ),\n",
       "  linear=Linear(\n",
       "    weight=f32[1,16],\n",
       "    bias=None,\n",
       "    in_features=16,\n",
       "    out_features=1,\n",
       "    use_bias=False\n",
       "  ),\n",
       "  bias=f32[1]\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(model, x, y)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 0.02099992, -0.22650257,  0.22024894, -0.14091924,  0.19867097,\n",
       "        0.186878  ,  0.2590352 , -0.19021241,  0.10789654, -0.00654481,\n",
       "        0.14347328, -0.12365455,  0.09235023,  0.01409564,  0.1799287 ,\n",
       "       -0.18900716, -0.08414254, -0.00932486,  0.08162478, -0.1449526 ,\n",
       "        0.20788766, -0.2012221 ,  0.05838855, -0.08605069, -0.15864289,\n",
       "        0.2008853 ,  0.08769115,  0.09258799, -0.12722623,  0.01412126,\n",
       "       -0.06657902,  0.03102791,  0.08074245,  0.21019915, -0.21532707,\n",
       "        0.18304779, -0.11230482,  0.15396012,  0.13632606,  0.17642733,\n",
       "       -0.00108753, -0.18435633,  0.0789031 ,  0.04088577,  0.04105607,\n",
       "        0.14619192,  0.08829977,  0.0833812 ], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cell.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 0.02033575, -0.22588035,  0.22095573, -0.14154696,  0.1980837 ,\n",
       "        0.18616064,  0.25889215, -0.1909398 ,  0.10857908, -0.00783069,\n",
       "        0.14367156, -0.1237536 ,  0.09237452,  0.01348901,  0.18081193,\n",
       "       -0.18829595, -0.08480729, -0.01026969,  0.08284629, -0.14506929,\n",
       "        0.20644861, -0.19957067,  0.05704628, -0.084939  , -0.1598957 ,\n",
       "        0.20243405,  0.08643414,  0.09179538, -0.12641516,  0.01319417,\n",
       "       -0.06741141,  0.03020173,  0.08141845,  0.21092674, -0.21462524,\n",
       "        0.18364033, -0.1130131 ,  0.15328297,  0.13565025,  0.17713504,\n",
       "       -0.00179389, -0.18505491,  0.0782076 ,  0.04022322,  0.04039116,\n",
       "        0.14551838,  0.08897749,  0.08271001], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eqx.apply_updates(model, updates).cell.bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_qdots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
